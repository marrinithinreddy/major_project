# major_project

One of the most crucial aspects of a person’s daily existence is their ability formation. The sighted could perform that action with easy, whereas blind people are significantly limited by their visual impairment. For blind people, navigation is essential because they can only use fixed routes when necessary for survival. The circumstances greatly hinder the life and employment of blind individuals. The blind is an important as well as a necessary part of our society and is given the same rights to make use of resources of the world. It is understood that a person’s inability to move independently can prevent them from participating in society as other. Disability such as blindness affects a person’s mobility and quality of life. Particularly when functional eyesight is lost. To make the blind community an active part of society, we present a blind navigation system based on image processing. It is an outdoor-based navigation system. It takes input through the camera and produces a voice message as output. It directs the person throughout the path and alerts when obstacles are head. The accessibility of GPS technology enables easy navigation.

Using modern technology, blind people can easily access materials that are available to everyone else. Depending on the situation, the person must travel to supermarkets, hospitals, train stations, etc. The blind can fulfil all of their demands thanks to the navigation system. Early days, several methods and tools are invented to help the blind. They are developed over time to help the blind in indoor and outdoor navigation such as humanitarian assistance, guide dog, and white cane. But the performance of these tools is not up to the mark. Based on technical advancements, a solution that is of less cost and is durable is offered. The technology had the potential to provide solutions to this realworld problem. It helps blinds in navigation indoor and outdoor environments. Users can get to their location with ease using the designed application. Users can use this application both inside and outside. They can access the needed object indoors. If they choose to use it outdoors, object detection will be combined with navigation, which will instruct the user to mention the desired location.

**Proposed Method and Implementation**

We used an integrated machine learning system to create a technological solution. It allows the blind to recognise items in real time and issues vocal commands to inform if the object’s near to them. In this application, the first question the user will be asked is whether they are going out or stay in home. If the voice input is ”home,” the application will only identify nearby objects and gives the voice output by name after detecting them. If the user voice input is ”going out,” the application will once more prompt the user for their intended destination. The application starts the journey with the walk path after receiving the destination location by redirecting to Google Maps. Object detection will be active in the background at the same time. The system is configured so that the programme communicates the taken image. The network server receives these collected photos. The SSD pretrained model is the one utilised by the system. The COCO DATASET is the training dataset. The blind are informed of an approaching object’s name by an alarm system. For feature extraction, it employs an architecture strategy based on RESNET. We are using SSD(Single Shot MultiBox Detector) beacuse, it is a specific kind of deep neural network created for tasks involving object detection. It is built on the idea of multi-box detection, which locates objects in an image by using a number of previously determined bounding boxes. The advantages of SSD include faster processing, accurate object localization, and effective memory usage. 

**Object Detection**

We had used TensorFlow Object Detection API, which is essentially a framework built for building a deep learning network, for object detection. The issues with object detection are resolved with this API. The user’s phone camera is used as the initial source of input. The system server receives these collected photos for object detection. For detecting obstacles, the SSD detection model is employed. This concept is predicated on the MOBILENET philosophy. The image is divided into grid cells by SSD, and each grid cell is in charge of identifying things in that area of the image. Predicting the type and location of an object within a region is essentially what it means to detect an object. By predicting the type and placement of an object in a specific zone, SSD makes it possible to detect things. It is measured which outputs are precise.

**API creation**

For straightforward implementation, the object detection code was saved on the server. The flask framework is used to establish an API for interacting with the code. The server and application are connected by this API.

**Depth estimation**

It seeks to determine how far away an obstruction is from a person in any given event occurring in real time. A rectangular box is created around the object once it has been discovered. A rough distance is determined using some limitations if the object takes up the majority of the frame. It provides the object’s location and distance information. Tensorflow sessions include essential components for detection. Use of boxes is employed. The width and centre of the items are measured using code.

Conditions:
If score greater than or equal to 50 percent, object is detected
If approximate distance is less than 0.5 and if mid greater than 0.3 and less than 0.7: object is too close

**Voice-generating module**

Text to speech conversion is possible with Pyttsx3, a Python conversion library programme called pyttsx.init(). OCR encodes and translates text material on images so that computers can read it with ease. Python Tesseract is used to read text images. A speech engine driver is loaded from the pyttsx.drivers module when a pyttsx.driver.driverproxy object is initialised by the engine during creation. To acquire and set speech engine properties, start and stop event loops, and register and unregistered call backs, an object is created by the engine and used by the application. As output, audio commands are produced. If an object is too close, its name will be output as audio.

**Conclusion**

In order to assist the blind in their daily activities, this study develops a navigation system as a technology solution. Both an indoor and outdoor navigation system is offered. The user uses voice instructions to specify the destination. The application reroutes to Google Maps and directs the user there. Using image processing, it also features obstacle detection. The user then receives alert messages about potential hazards, their location, and directions. It is inexpensive and simple to use. After testing the application, it got a accuracy of 92% With the addition of a face recognition system and the deployment of object detection code into the application, we can able to improve accuracy. As a result, it would make it possible for the user to identify his friends and family who had previously registered for the application. In order to help the user identify the person in his immediate surroundings, this information can also be converted to voice.
